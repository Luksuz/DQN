{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TicTacToe import TicTacToe\n",
    "env = TicTacToe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = [1, 1]\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action_space': array([ 0,  1, -1,  0,  0,  0,  0,  0,  0])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1, -1],\n",
       "       [ 0,  0,  0],\n",
       "       [ 0,  0,  0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1, -1,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = [1, 0]\n",
    "observation, reward, terminated, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1, -1],\n",
       "       [-1,  0,  0],\n",
       "       [ 0,  0,  0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.available_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid action: Position already occupied.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m action \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/DQN/tictactoe_py/TicTacToe.py:47\u001b[0m, in \u001b[0;36mTicTacToe.step\u001b[0;34m(self, action, human_opponent)\u001b[0m\n\u001b[1;32m     45\u001b[0m mark, field \u001b[38;5;241m=\u001b[39m action\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space[field] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid action: Position already occupied.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_mark(action)\n\u001b[1;32m     49\u001b[0m terminated, winner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_board()\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid action: Position already occupied."
     ]
    }
   ],
   "source": [
    "action = [1, 3]\n",
    "observation, reward, terminated, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((np.int64(1),\n",
       "  np.int64(1),\n",
       "  np.int64(-1),\n",
       "  np.int64(-1),\n",
       "  np.int64(0),\n",
       "  np.int64(0),\n",
       "  np.int64(0),\n",
       "  np.int64(0),\n",
       "  np.int64(0)),\n",
       " -1,\n",
       " False,\n",
       " False,\n",
       " {'action_space': array([ 1,  1, -1, -1,  0,  0,  0,  0,  0])})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward, terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, lr, initial_epsilon, epsilon_decay, min_epsilon, discout=0.95):\n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.shape))\n",
    "        self.lr = lr\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.discount_factor = discout\n",
    "\n",
    "        self.training_error = []\n",
    "\n",
    "    def get_action(self, obs, env):\n",
    "        # if len(env.available_indices) >= 8 and env.available_indices[4] == 0:\n",
    "        #     return [1, 4]\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return [1, np.random.choice(env.available_indices)]\n",
    "\n",
    "        else:\n",
    "            q_values = self.q_values[obs]\n",
    "            valid_actions = [(i, q_values[i]) for i in env.available_indices]\n",
    "            action = max(valid_actions, key=lambda x: x[1])[0]\n",
    "            return [1, action]\n",
    "\n",
    "    def update(self, obs: Tuple[int], action, reward, terminated, next_obs: Tuple[int]):\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "        temporal_difference = reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
    "\n",
    "        self.q_values[obs][action] += self.lr * temporal_difference\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon - self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(lr=0.1, initial_epsilon=1, epsilon_decay=2e-6, min_epsilon=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1000000\n",
    "wins, epsilons = 0, []\n",
    "epochs = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:   5%|▌         | 50503/1000000 [00:17<05:10, 3062.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50000 - Epsilon: 0.900000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:  10%|█         | 100519/1000000 [00:33<05:28, 2741.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000 - Epsilon: 0.800000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:  15%|█▌        | 150212/1000000 [00:49<05:10, 2739.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 150000 - Epsilon: 0.700000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:  20%|██        | 200543/1000000 [01:05<03:58, 3354.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 200000 - Epsilon: 0.600000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:  25%|██▌       | 250409/1000000 [01:20<03:34, 3491.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 250000 - Epsilon: 0.500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:  30%|███       | 300490/1000000 [01:35<03:18, 3526.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 300000 - Epsilon: 0.400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:  35%|███▌      | 350383/1000000 [01:49<04:38, 2331.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 350000 - Epsilon: 0.300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:  40%|████      | 400743/1000000 [02:03<02:34, 3874.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 400000 - Epsilon: 0.200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:  45%|████▌     | 450450/1000000 [02:16<03:23, 2705.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 450000 - Epsilon: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:  50%|█████     | 500446/1000000 [02:29<02:12, 3766.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 500000 - Epsilon: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:  55%|█████▌    | 550569/1000000 [02:41<01:47, 4183.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 550000 - Epsilon: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:  60%|██████    | 600997/1000000 [02:54<01:35, 4157.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 600000 - Epsilon: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:  65%|██████▌   | 650431/1000000 [03:05<01:23, 4178.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 650000 - Epsilon: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:  70%|███████   | 700567/1000000 [03:18<01:11, 4215.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 700000 - Epsilon: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:  75%|███████▌  | 750717/1000000 [03:30<00:59, 4196.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 750000 - Epsilon: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:  80%|████████  | 800422/1000000 [03:42<00:47, 4187.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 800000 - Epsilon: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:  85%|████████▌ | 850576/1000000 [03:54<00:36, 4141.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 850000 - Epsilon: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:  90%|█████████ | 900631/1000000 [04:06<00:23, 4201.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 900000 - Epsilon: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:  95%|█████████▌| 950544/1000000 [04:18<00:12, 3852.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 950000 - Epsilon: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes: 100%|██████████| 1000000/1000000 [04:30<00:00, 3696.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1000000 - Epsilon: 0.010000\n",
      "Total wins: 795035\n",
      "Win ratio: 79.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "wins = 0  # Initialize wins counter\n",
    "epsilons = []  # List to store epsilon values over episodes\n",
    "\n",
    "# Training loop\n",
    "for episode in tqdm(range(n_episodes), desc=\"Training Episodes\"):\n",
    "    \n",
    "    if episode % 2 == 0:\n",
    "        obs, info = env.reset(player2_advantage=True)\n",
    "    else: \n",
    "        obs, info = env.reset(player2_advantage=False)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.get_action(obs, env)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        done = terminated\n",
    "        if done and reward == 1:\n",
    "            wins += 1\n",
    "\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "        obs = next_obs\n",
    "\n",
    "    epsilons.append(agent.epsilon)\n",
    "    agent.decay_epsilon()\n",
    "\n",
    "    if (episode + 1) % 50000 == 0:\n",
    "        print(f\"Episode: {episode + 1} - Epsilon: {agent.epsilon:.6f}\")\n",
    "\n",
    "# Final results\n",
    "print(f\"Total wins: {wins}\")\n",
    "win_ratio = (wins / (n_episodes)) * 100\n",
    "print(f\"Win ratio: {win_ratio:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes: 100%|██████████| 10000/10000 [00:02<00:00, 3893.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total wins: 9774\n",
      "Win ratio: 97.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wins = 0  # Initialize wins counter\n",
    "epsilons = []  # List to store epsilon values over episodes\n",
    "\n",
    "# Training loop\n",
    "for episode in tqdm(range(10000), desc=\"Training Episodes\"):\n",
    "    obs, info = env.reset(player2_advantage=False)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.get_action(obs, env)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        done = terminated\n",
    "        if done and reward == 1:\n",
    "            wins += 1\n",
    "\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "        obs = next_obs\n",
    "\n",
    "    epsilons.append(agent.epsilon)\n",
    "    agent.decay_epsilon()\n",
    "\n",
    "print(f\"Total wins: {wins}\")\n",
    "win_ratio = (wins / (10000)) * 100\n",
    "print(f\"Win ratio: {win_ratio:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1 -1]\n",
      "[1 0 0]\n",
      "[ 1 -1  0]\n"
     ]
    }
   ],
   "source": [
    "action = agent.get_action(obs, env)\n",
    "next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "done = terminated\n",
    "\n",
    "agent.update(obs, action, reward, terminated, next_obs)\n",
    "obs = next_obs\n",
    "\n",
    "env.display_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1 -1]\n",
      "[1 0 0]\n",
      "[ 1 -1  0]\n"
     ]
    }
   ],
   "source": [
    "env.display_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3056"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agent.q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "byte_size = sys.getsizeof(agent.q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of q_table is: 0.281341552734375 MB\n"
     ]
    }
   ],
   "source": [
    "mb_size = byte_size / (1024 * 1024)  # Convert bytes to megabytes\n",
    "print(f\"Size of q_table is: {mb_size} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m obs, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset(player2_advantage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(obs, env)\n\u001b[0;32m----> 4\u001b[0m next_obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhuman_opponent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated\n\u001b[1;32m      8\u001b[0m agent\u001b[38;5;241m.\u001b[39mupdate(obs, action, reward, terminated, next_obs)\n",
      "File \u001b[0;32m~/Desktop/DQN/TicTacToe.py:58\u001b[0m, in \u001b[0;36mTicTacToe.step\u001b[0;34m(self, action, human_opponent)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, info\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m human_opponent:\n\u001b[0;32m---> 58\u001b[0m     agent_2_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter an unoccupied indec on the board: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     agent_2_action \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_indices))\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "obs, info = env.reset(player2_advantage=True)\n",
    "\n",
    "action = agent.get_action(obs, env)\n",
    "next_obs, reward, terminated, truncated, info = env.step(action, human_opponent=True)\n",
    "\n",
    "done = terminated\n",
    "\n",
    "agent.update(obs, action, reward, terminated, next_obs)\n",
    "obs = next_obs\n",
    "\n",
    "env.display_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 4, 6],\n",
       "       [2, 4, 6]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "\n",
    "b = np.array([[1,2,3], \n",
    "             [1,2,3]])\n",
    "\n",
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = dict(agent.q_values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"q_table.pkl\", \"wb\") as f:\n",
    "    pickle.dump(q_table, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "b = a\n",
    "del a  # Deletes the reference 'a', but the list still exists as 'b' references it\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(-1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(-1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(-1))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
